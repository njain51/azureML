{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Training a Model on Remote Compute\n",
    "\n",
    "In this notebook, you will train a model on remote compute powered by Azure Machine Learning. While we will be working primarily in this Jupyter notebook, the actual training will be done on a separate compute cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We first need to import several modules from the Azure Machine Learning Python SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.opendatasets import MNIST\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Experiment\n",
    "\n",
    "The first step toward the goal of training our model is to create an experiment.  The documentation defines an experiment in the following manner:\n",
    "\n",
    "> Azure Machine Learning experiment represent the collection of trials used to validate a user's hypothesis. [Experiment Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py)\n",
    "\n",
    "We will first need to get the reference to our workspace, and then we will create an experiment with the name `keras-mnist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reference to the workspace\n",
    "ws = Workspace.from_config()\n",
    "print(\"Azure ML Workspace\")\n",
    "print(f'Name: {ws.name}')\n",
    "print(f'Location: {ws.location}')\n",
    "print(f'Resource Group: {ws.resource_group}')\n",
    "\n",
    "# Create an experiment, or get a reference to the experiment if it already exists\n",
    "experiment_name = 'keras-mnist'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "print(\"Azure ML Experiment\")\n",
    "print(f'ID: {exp.id}')\n",
    "print(f'Name: {exp.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a directory that will hold the script we will use for training our model.  This directory will eventually be sent to our remote compute cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"keras-mnist\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to output the script that we will use to train our model.  It will have the name `train.py` in our `keras-mnist` directory.\n",
    "\n",
    "Within an experiment in Azure Machine Learning, each time you execute a test on your experiment, it is considered a `Run`.  The documentation defines this as:\n",
    "\n",
    "> A run represents a single trial of an experiment. Runs are used to monitor the asynchronous execution of a trial, log metrics and store output of the trial, and to analyze results and access artifacts generated by the trial. [Run Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run(class)?view=azure-ml-py)\n",
    "\n",
    "We will need to get the context of the current `Run` from within our training script. We can use this to register metrics from our test and make them associated with that specific `Run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from azureml.core import Run\n",
    "from utils import load_data\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "print(f'TensorFlow Version: {tf.__version__}')\n",
    "print(f'Keras Version: {keras.__version__}')\n",
    "\n",
    "# ARGUMENT HANDLING\n",
    "# Read in the two arguments that are passed into the run\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--epochs', type=int, dest='epochs', default=10, help='number of epochs')\n",
    "args = parser.parse_args()\n",
    "\n",
    "epochs = args.epochs\n",
    "data_folder = args.data_folder\n",
    "print(f'Data folder: {data_folder}')\n",
    "print(f'Epochs: {epochs}')\n",
    "\n",
    "# LOAD DATA\n",
    "# We will leverage a slightly modified version of the code that we used to load the data in the\n",
    "# previous notebook\n",
    "num_classes = 10\n",
    "\n",
    "training_images = load_data(data_folder, \"train-images-idx3-ubyte.gz\", False) / 255.0\n",
    "training_images = np.reshape(training_images, (-1, 28,28)).astype('float32')\n",
    "test_images = load_data(data_folder, \"t10k-images-idx3-ubyte.gz\", False) / 255.0\n",
    "test_images = np.reshape(test_images, (-1, 28,28)).astype('float32')\n",
    "\n",
    "training_labels = load_data(data_folder, \"train-labels-idx1-ubyte.gz\", True).reshape(-1)\n",
    "training_labels = to_categorical(training_labels, num_classes)\n",
    "test_labels = load_data(data_folder, \"t10k-labels-idx1-ubyte.gz\", True).reshape(-1)\n",
    "test_labels = to_categorical(test_labels, num_classes)\n",
    "\n",
    "print(f'Training Image: {training_images.shape}')\n",
    "print(f'Training Labels: {training_labels.shape}')\n",
    "print(f'Test Images: {test_images.shape}')\n",
    "print(f'Test Labels: {test_labels.shape}')\n",
    "\n",
    "# RUN CONTEXT\n",
    "# We need to get a reference to the current run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# MODEL CREATION & EVALUATION\n",
    "# We next need to create, compile, fit, and evaluate our model\n",
    "model_data = training_images, training_labels, test_images, test_labels\n",
    "\n",
    "model = keras.Sequential([\n",
    "    Flatten(input_shape=training_images[0].shape),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "input_shape = training_images.shape\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "          optimizer=tf.keras.optimizers.Adam(0.001),\n",
    "          metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels,\n",
    "      batch_size=128,\n",
    "      epochs=epochs,\n",
    "      verbose=1,\n",
    "      validation_data=(test_images, test_labels))\n",
    "\n",
    "score = model.evaluate(test_images, test_labels, verbose=0)\n",
    "\n",
    "# METRICS\n",
    "# We need to associate our accuracy metric with the Run context\n",
    "print('Test accuracy:', score[1])\n",
    "run.log('accuracy', score[1])\n",
    "\n",
    "# MODEL EXPORT\n",
    "# We will save out our model export so that we can register this within our workspace\n",
    "os.makedirs('outputs/model', exist_ok=True)\n",
    "model.save('outputs/model/mnist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our training script will also need to access our `load_data` function, we need to be sure to copy the `utils.py` file into this directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('utils.py', script_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Estimators\n",
    "\n",
    "To submit our training script to our compute cluster, we need to create an `Estimator` instance.  The documentation defines an `Estimator` as:\n",
    "\n",
    "> To facilitate deep learning model training, the Azure Machine Learning Python SDK provides an alternative higher-level abstraction, the estimator class, which allows users to easily construct run configurations. You can create and use a generic Estimator to submit training script using any learning framework you choose (such as scikit-learn) on any compute target you choose, whether it's your local machine, a single VM in Azure, or a GPU cluster in Azure. For PyTorch, TensorFlow and Chainer tasks, Azure Machine Learning also provides respective PyTorch, TensorFlow, and Chainer estimators to simplify using these frameworks. [Train models with Azure Machine Learning using estimator](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-train-ml-models)\n",
    "\n",
    "We will specifically be leveraging the `TensorFlow` Estimator class [see documentation](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn.tensorflow?view=azure-ml-py) since we are leveraging Keras on top of TensorFlow.\n",
    "\n",
    "To create our estimator, we will need the following:\n",
    "\n",
    "1. Environment Configuration\n",
    "1. Script Parameters\n",
    "1. Script directory and entry point\n",
    "1. Compute cluster and configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Script Parameters\n",
    "\n",
    "We will need to include two different parameters in our script parameters: the data folder, and the number of epochs for model compilation.  We can include any parameters here, and we can then leverage these to gauge model effectiveness with different parameter customizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_file_dataset = MNIST.get_file_dataset()\n",
    "\n",
    "script_params = {\n",
    "    # to mount files referenced by mnist dataset\n",
    "    '--data-folder': mnist_file_dataset.as_named_input('mnist_opendataset').as_mount(),\n",
    "    '--epochs': 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remaining Estimator Configuration\n",
    "\n",
    "We will include our compute cluster plus its configuration as well as the script directory and entry point as we create our estimator instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_cluster = AmlCompute(workspace=ws, name='tdsp-cluster')\n",
    "\n",
    "est = TensorFlow(source_directory=script_folder,\n",
    "                 entry_script='train.py',\n",
    "                 script_params=script_params,\n",
    "                 compute_target=cpu_cluster,\n",
    "                 framework_version='2.0',\n",
    "                 pip_packages=['azureml-dataprep[fuse,pandas]','keras==2.2.4'],\n",
    "                 use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting our First Run\n",
    "\n",
    "We can then submit an Estimator experiment which will create a instance of the `Run` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = exp.submit(config=est)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this will take some time to complete, we can use an included utility to see the progress within our Jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once it completes, we can extract the metrics from this run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(run.get_metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering Model\n",
    "\n",
    "Finally, we can register our model in our workspace, so that it can be leveraged elsewhere.  We will use this model in the next clip to deploy it as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register model \n",
    "from azureml.core import Model\n",
    "from azureml.core.resource_configuration import ResourceConfiguration\n",
    "\n",
    "model = run.register_model(model_name='keras-mnist', \n",
    "                           model_path='outputs/model/mnist.h5',\n",
    "                           model_framework=Model.Framework.TENSORFLOW,\n",
    "                           model_framework_version='2.0',\n",
    "                           resource_configuration=ResourceConfiguration(cpu=1, memory_in_gb=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
