{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Deploy a Model with GPU Support\n",
    "\n",
    "In this notebook we will deploy our existing model as a webservice with GPU support. Since GPU support is needed, we will be leveraging [Azure Kubernetes Service (AKS)](https://azure.microsoft.com/en-us/services/kubernetes-service/) instead of [Azure Container Instances (ACI)](https://azure.microsoft.com/en-us/services/container-instances/) which we leveraged previously.\n",
    "\n",
    "## Important Note on Cost\n",
    "\n",
    "This notebook will spin up a GPU cluster for AKS.  This cluster will incur charges from the moment you start it.  Because of this, **make sure you delete the service and cluster**.  The steps to do that are at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "We first need to import some modules before we can launch our GPU compute capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.exceptions imp\n",
    "ort ComputeTargetException\n",
    "from azureml.core.compute import ComputeTarget, AksCompute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to get a reference to our workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the workspace\n",
    "ws = Workspace.from_config()\n",
    "print(\"Azure ML Workspace\")\n",
    "print(f'Name: {ws.name}')\n",
    "print(f'Location: {ws.location}')\n",
    "print(f'Resource Group: {ws.resource_group}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Compute Cluster\n",
    "\n",
    "The next step will be a new compute cluster with GPU capabilities targeted for use with Azure Kubernetes Service (AKS).  **In most cases, this step will take 10-25 minutes to complete**.\n",
    "\n",
    "### Quotas\n",
    "\n",
    "You may receive an error related to quotas when trying to launch this cluster.  If this happens, you can follow these steps to request an increase:\n",
    "\n",
    "1. From the Azure Portal, navigate to your subscription.\n",
    "1. Select the `Usage + Quotas` option from the menu.\n",
    "1. Select the `Request Increase` button.\n",
    "1. In the form, fill in the fields with your information. Select `Compute-VM (cores-vCPUs) subscription limit increases` for the `Quota Type`. Select `Next`.\n",
    "1. In the next view, fill out the information and then press the `Provide Details` option at the top of the form.\n",
    "1. In this modal, fill out the details to match the following image (edit specific for your location and use case).\n",
    "1. Submit your request.\n",
    "\n",
    "<img src=\"quota.png\" alt=\"Request Quota Increase\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a name for our new cluster\n",
    "gpu_cluster_name = 'gpu-cluster'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    gpu_cluster = AksCompute(workspace=ws, \n",
    "                             name=gpu_cluster_name)\n",
    "    print('Cluster already exists.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AksCompute.provisioning_configuration(vm_size=\"Standard_NC6_Promo\")\n",
    "    gpu_cluster = AksCompute.create(ws, \n",
    "                                    name=gpu_cluster_name,\n",
    "                                    provisioning_configuration=compute_config)\n",
    "\n",
    "gpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring File\n",
    "\n",
    "Next, we need to create the scoring file that will be leveraged by the webservice.  Just as before, we have to implement both the `init` and the `run` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score_gpu.py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import load_model\n",
    "\n",
    "# Initialize the model\n",
    "def init():\n",
    "    global model\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'mnist.h5')\n",
    "    model = load_model(model_path)\n",
    "\n",
    "# Run inference against data that is passed in\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # make prediction\n",
    "    results = model.predict(data)\n",
    "    output = []\n",
    "    for result in results:\n",
    "        output.append(construct_output(result))\n",
    "    return output\n",
    "\n",
    "# Utility function to construct output data per item passed in\n",
    "def construct_output(result):\n",
    "    result_index = np.argmax(result)\n",
    "    result_value = result[result_index]\n",
    "    output = { 'value': str(result_index) }\n",
    "    output['certainty'] = result[result_index].item()\n",
    "    possibilities = {}\n",
    "    for i, val in enumerate(result): \n",
    "        possibilities[i] = val.item() \n",
    "    output['possibilities'] = possibilities    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment\n",
    "\n",
    "Next, we need to configure the Environment for our webservice.  In this case, we want to be sure to include the GPU version of TensorFlow by adding the conda package `tensorflow-gpu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"tensorflow-gpu\")\n",
    "myenv.add_conda_package(\"keras\")\n",
    "\n",
    "with open(\"gpuenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "    \n",
    "# Review environment file\n",
    "with open(\"gpuenv.yml\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment & Inference Configuration\n",
    "\n",
    "Next, we need to create our deployment configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice\n",
    "\n",
    "gpu_aks_config = AksWebservice.deploy_configuration(autoscale_enabled=False,\n",
    "                                                    num_replicas=3,\n",
    "                                                    cpu_cores=2,\n",
    "                                                    memory_gb=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can configure our `InferenceConfig` for the webservice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(runtime=\"python\",\n",
    "                                   entry_script=\"score_gpu.py\",\n",
    "                                   conda_file=\"gpuenv.yml\",\n",
    "                                   enable_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy our Model\n",
    "\n",
    "The final step in the process is to deploy our model using the configuration that we have put in place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "aks_service_name = 'keras-mnist-gpu-svc'\n",
    "# Get our model\n",
    "model = Model(ws, \"keras-mnist\")\n",
    "# Deploy our model\n",
    "aks_service = Model.deploy(ws,\n",
    "                           models=[model],\n",
    "                           inference_config=inference_config,\n",
    "                           deployment_config=gpu_aks_config,\n",
    "                           deployment_target=gpu_cluster,\n",
    "                           name=aks_service_name)\n",
    "\n",
    "aks_service.wait_for_deployment(show_output=True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can get the URL for the webservice endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Scoring URL: {aks_service.scoring_uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate our Deployment\n",
    "\n",
    "Next, we want to validate our deployment using both the SDK and HTTP calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "First, we will need to download our data locally so that we can submit it to the service:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# DATA FOLDER\n",
    "# Make sure we have the data folder created locally\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# LOAD DATA\n",
    "training_images = load_data(os.path.join(data_folder, \"train-images-idx3-ubyte.gz\"), False) / 255.0\n",
    "training_images = np.reshape(training_images, (-1, 28,28)).astype('float32')\n",
    "test_images = load_data(os.path.join(data_folder, \"t10k-images-idx3-ubyte.gz\"), False) / 255.0\n",
    "test_images = np.reshape(test_images, (-1, 28,28)).astype('float32')\n",
    "\n",
    "training_labels = load_data(os.path.join(data_folder, \"train-labels-idx1-ubyte.gz\"), True).reshape(-1)\n",
    "test_labels = load_data(os.path.join(data_folder, \"t10k-labels-idx1-ubyte.gz\"), True).reshape(-1)\n",
    "\n",
    "print(f'Training Image: {training_images.shape}')\n",
    "print(f'Training Labels: {training_labels.shape}')\n",
    "print(f'Test Images: {test_images.shape}')\n",
    "print(f'Test Labels: {test_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with SDK\n",
    "\n",
    "Next,we can validate our service using the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Get a sample index\n",
    "sample_indices = np.random.permutation(test_images.shape[0])[0:1]\n",
    "\n",
    "# Structure input data\n",
    "test_samples = json.dumps({\"data\": test_images[sample_indices].tolist()})\n",
    "print(\"JSON Input: \" + test_samples)\n",
    "test_samples = bytes(test_samples, encoding='utf8')\n",
    "\n",
    "# Execute the predictions\n",
    "results = aks_service.run(input_data=test_samples)\n",
    "\n",
    "# Utility function to display the result\n",
    "def display_result(image, result):\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    grid = plt.GridSpec(1, 3, wspace=0.4, hspace=0.3)\n",
    "    plt.subplot(grid[0, 0])\n",
    "    plt.imshow(image, cmap='gray_r')\n",
    "    print(\"\\n\\n\")\n",
    "    print(f'Predicted Value: {result[\"value\"]}')\n",
    "    print(f'Certainty: {str(result[\"certainty\"])}')\n",
    "    print(f'Raw Result: {str(result)}')\n",
    " \n",
    "# Show the results\n",
    "for i, val in enumerate(results):\n",
    "    display_result(test_images[sample_indices[i]], val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate over HTTP\n",
    "\n",
    "Finally, we can validate our deployment using an HTTP call.  Our first step in this process is to get the API key for the AKS service and add that to the headers for our call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get API Key and Construct Headers for call\n",
    "api_key = aks_service.get_keys()[0]\n",
    "headers = {'Content-Type': 'application/json',\n",
    "           'Authorization': ('Bearer ' + api_key)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that in place, we can now perform a `POST` request against our inference endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Get a sample index\n",
    "sample_indices = np.random.permutation(test_images.shape[0])[0:1]\n",
    "    \n",
    "# Structure input data\n",
    "test_samples = json.dumps({\"data\": test_images[sample_indices].tolist()})\n",
    "print(\"JSON Input: \" + test_samples)\n",
    "test_samples = bytes(test_samples, encoding='utf8')\n",
    "\n",
    "# Perform a POST request\n",
    "resp = requests.post(aks_service.scoring_uri, test_samples, headers=headers)\n",
    "\n",
    "print(\"POST to url\", aks_service.scoring_uri)\n",
    "\n",
    "# Read the returned JSON data\n",
    "results = json.loads(resp.text)\n",
    "\n",
    "# Show the results\n",
    "for i, val in enumerate(results):\n",
    "    display_result(test_images[sample_indices[i]], val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Service\n",
    "\n",
    "Given the cost of running this cluster, you will want to delete both the service and the cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_service.delete()\n",
    "gpu_cluster.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
