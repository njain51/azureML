{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Creating an Azure ML Pipeline\n",
    "\n",
    "This notebook will create a pipeline within the Azure Machine Learning service that prepares data, trains a model, and registers the model within a workspace.\n",
    "\n",
    "This pipeline can sit within a larger pipeline that integrates model deployment as well as continuous delivery for the code that is leveraged.  While that is beyond the scope of this notebook, you can learn more about this overall approach here:\n",
    "\n",
    "[MLOps-Python Reference Architecture](https://github.com/microsoft/MLOpsPython)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "First, we will need to update several modules for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace, Datastore\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.opendatasets import MNIST\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "We will need to get a reference to both the Azure ML workspace as well as the current experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a reference to the workspace\n",
    "ws = Workspace.from_config()\n",
    "print(\"Azure ML Workspace\")\n",
    "print(f'Name: {ws.name}')\n",
    "print(f'Location: {ws.location}')\n",
    "print(f'Resource Group: {ws.resource_group}')\n",
    "\n",
    "# Create an experiment, or get a reference to the experiment if it already exists\n",
    "experiment_name = 'keras-mnist'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)\n",
    "print(\"Azure ML Experiment\")\n",
    "print(f'ID: {exp.id}')\n",
    "print(f'Name: {exp.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need to get a reference to the compute cluster we are leveraging from the workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a name for our new cluster\n",
    "cpu_cluster_name = 'tdsp-cluster'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = AmlCompute(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Cluster already exists.')\n",
    "    \n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6',\n",
    "                                                           max_nodes=4)\n",
    "    cpu_cluster = AmlCompute.create(ws, cpu_cluster_name, compute_config)\n",
    "\n",
    "cpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Creation\n",
    "\n",
    "Now that we have the needed references in place, we will need to create the actual pipeline.\n",
    "\n",
    "### Imports\n",
    "\n",
    "First, we will need to import some additional modules that are specific to our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Package Dependencies\n",
    "\n",
    "Next, we need to define the `RunConfiguration` that will be used to run the pipeline steps.  In this case, we need to be sure that we include `tensorflow` and `keras` from conda and `azureml-opendatasets` from pip (which provides the MNIST dataset that we have been working with). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE our conda and pip dependencies for the pipeline environment\n",
    "conda_dependencies = CondaDependencies.create(conda_packages=['tensorflow','keras'])\n",
    "conda_dependencies.add_pip_package('azureml-opendatasets')\n",
    "run_config = RunConfiguration(conda_dependencies=conda_dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Storage and Inputs/Outputs\n",
    "\n",
    "Next, we need to define where we will store the outputs of each pipeline step.  In this case, we will be storing this in blob storage.  In addition, we need to define the two `PipelineData` instances: the prepared data and the compiled model.\n",
    "\n",
    "We will be leveraging the `upload` mode on both of these `PipelineData` instances, which means that they will be uploaded after out step completes to the datastore.  These will then be made available to subsequent steps that require them as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blob storage associated with the workspace\n",
    "blob_store = Datastore(ws, \"workspaceblobstore\")\n",
    "\n",
    "# Create our Pipeline Data references\n",
    "mnist_data = PipelineData(\"mnist_data\",\n",
    "                          datastore=blob_store,\n",
    "                          output_mode=\"upload\",\n",
    "                          output_path_on_compute=\"data/mnist.npy\",\n",
    "                          output_overwrite=True)\n",
    "\n",
    "model_data = PipelineData(\"mnist_model\",\n",
    "                          datastore=blob_store,\n",
    "                          output_mode=\"upload\",\n",
    "                          output_path_on_compute=\"outputs/mnist.h5\",\n",
    "                          output_overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Steps\n",
    "\n",
    "Next, we are ready to define our pipeline steps.  In this case, we chose to make each step a `PythonScriptStep`. We could have chosen to use the `EstimatorStep` for training, but for simplicity we will not use it in this initial pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where scripts reside\n",
    "scripts_directory = './pipeline'\n",
    "\n",
    "# Create Pipeline Steps\n",
    "prep_data_step = PythonScriptStep(name=\"Prep Data\",\n",
    "                                  script_name=\"prepData.py\",\n",
    "                                  compute_target=cpu_cluster,\n",
    "                                  outputs=[mnist_data],\n",
    "                                  source_directory=scripts_directory,\n",
    "                                  runconfig=run_config)\n",
    "\n",
    "train_step = PythonScriptStep(name=\"Train Model\",\n",
    "                              script_name=\"train.py\",\n",
    "                              arguments=[\"--input-data\", mnist_data],\n",
    "                              compute_target=cpu_cluster,\n",
    "                              inputs=[mnist_data],\n",
    "                              outputs=[model_data],\n",
    "                              source_directory=scripts_directory,\n",
    "                              runconfig=run_config)\n",
    "\n",
    "register_step = PythonScriptStep(name=\"Register Model\",\n",
    "                                 script_name=\"register.py\",\n",
    "                                 arguments=[\"--input-data\", model_data],\n",
    "                                 compute_target=cpu_cluster,\n",
    "                                 inputs=[model_data],\n",
    "                                 source_directory=scripts_directory,\n",
    "                                 runconfig=run_config)\n",
    "\n",
    "pipeline_steps = [prep_data_step, train_step, register_step]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run the Pipeline\n",
    "\n",
    "Now that we have created the pipeline steps, we will now create the instance of the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=pipeline_steps)\n",
    "print (\"Pipeline is built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "Before we execute the pipeline, we can validate the configuration by calling the `validate` method on our pipeline instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.validate()\n",
    "print(\"Pipeline validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "Since we have a validated pipeline instance, we can now execute the pipeline.  We can utilize the same `RunDetails` utility to track the progress of the pipeline run directly from our notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = exp.submit(pipeline)\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
