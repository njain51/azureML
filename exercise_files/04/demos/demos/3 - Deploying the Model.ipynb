{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Deploying the Model\n",
    "\n",
    "The final step in this module will be to deploy that we created in the previous clip and make it available to call via either the SDK or through a REST endpoint.  This will allow us to operationalize this model within our organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "import azureml.core\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating our Model Locally\n",
    "\n",
    "In the previous notebook we trained our model, and then we registered it within our workspace.  We now need to make sure we can pull this model down to our notebook server and use it there.  This is a critical step to ensure that we diagnose any problems here before we deploy this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Get a reference to our workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get a reference to our model\n",
    "amlModel=Model(ws, 'keras-mnist')\n",
    "\n",
    "# Download the model to our local notebook server\n",
    "amlModel.download(target_dir=os.getcwd(), exist_ok=True)\n",
    "\n",
    "# Make sure that the file downloaded\n",
    "file_path = os.path.join(os.getcwd(), \"mnist.h5\")\n",
    "os.stat(file_path)\n",
    "\n",
    "# Use Keras to load this model\n",
    "model = load_model('mnist.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our model downloaded locally, we can now verify that we can leverage it for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data\n",
    "\n",
    "# DATA FOLDER\n",
    "# Make sure we have the data folder created locally\n",
    "data_folder = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# LOAD DATA\n",
    "# We use a slightly modified version of the logic we have for loading in our data\n",
    "num_classes = 10\n",
    "\n",
    "training_images = load_data(data_folder, \"train-images-idx3-ubyte.gz\", False) / 255.0\n",
    "training_images = np.reshape(training_images, (-1, 28,28)).astype('float32')\n",
    "test_images = load_data(data_folder, \"t10k-images-idx3-ubyte.gz\", False) / 255.0\n",
    "test_images = np.reshape(test_images, (-1, 28,28)).astype('float32')\n",
    "\n",
    "training_labels = load_data(data_folder, \"train-labels-idx1-ubyte.gz\", True).reshape(-1)\n",
    "test_labels = load_data(data_folder, \"t10k-labels-idx1-ubyte.gz\", True).reshape(-1)\n",
    "\n",
    "print(f'Training Image: {training_images.shape}')\n",
    "print(f'Training Labels: {training_labels.shape}')\n",
    "print(f'Test Images: {test_images.shape}')\n",
    "print(f'Test Labels: {test_labels.shape}')\n",
    "\n",
    "# CLASS NAMES\n",
    "# Get the Class Names from the labels\n",
    "class_names = np.unique(training_labels)\n",
    "\n",
    "# PREDICT\n",
    "# Provide the array of predictions for the passed in image\n",
    "def predict_image(image):\n",
    "    image_test = (np.expand_dims(image,0))\n",
    "    return model.predict(image_test)[0]\n",
    "\n",
    "# VISUALIZE\n",
    "# Function to predict an image based on the model and visualize predictions\n",
    "def visualize_image_prediction(image):\n",
    "    predictions = predict_image(image)\n",
    "    fig = plt.figure(figsize=(18, 5))\n",
    "    grid = plt.GridSpec(1, 3, wspace=0.4, hspace=0.3)\n",
    "    plt.subplot(grid[0, 0])\n",
    "    plt.imshow(image, cmap='gray_r')\n",
    "    plt.subplot(grid[0, 1:])\n",
    "    plt.bar(range(10), predictions, color=\"#f05a28\")\n",
    "    plt.xticks(range(10), class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this logic in place, we can test against 5 of the training images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    visualize_image_prediction(training_images[i+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model as Web Service\n",
    "\n",
    "Now that we have validated that the model is working locally, we can transition to deploying the model as a web service.\n",
    "\n",
    "### Configuring the Web Service\n",
    "\n",
    "The first step is to create a script that will be called when the service is executed.  We will call this script `score.py`.\n",
    "\n",
    "There are two functions that we must implement in this file:\n",
    "\n",
    "* `init` - this function will handle the loading of the model\n",
    "* `run` - this will handle running inference against the data that is passed in with each call\n",
    "\n",
    "You can see an implementation of this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Initialize the model\n",
    "def init():\n",
    "    global model\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'mnist.h5')\n",
    "    model = load_model(model_path)\n",
    "\n",
    "# Run inference against data that is passed in\n",
    "def run(raw_data):\n",
    "    data = np.array(json.loads(raw_data)['data'])\n",
    "    # make prediction\n",
    "    results = model.predict(data)\n",
    "    output = []\n",
    "    for result in results:\n",
    "        output.append(construct_output(result))\n",
    "    return output\n",
    "\n",
    "# Utility function to construct output data per item passed in\n",
    "def construct_output(result):\n",
    "    result_index = np.argmax(result)\n",
    "    result_value = result[result_index]\n",
    "    output = { 'value': str(result_index) }\n",
    "    output['certainty'] = result[result_index].item()\n",
    "    possibilities = {}\n",
    "    for i, val in enumerate(result): \n",
    "        possibilities[i] = val.item() \n",
    "    output['possibilities'] = possibilities    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we configure the environment that our service will be running in.  This will include adding conda packages for both `tensorflow` and `keras`.  We "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies()\n",
    "myenv.add_conda_package(\"tensorflow\")\n",
    "myenv.add_conda_package(\"keras\")\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())\n",
    "    \n",
    "# Review environment file\n",
    "with open(\"myenv.yml\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once that is in place, we can create the deploy configuration, which will be created leveraging the `AciWebservice` class.  This will allow our webservice to be launched on Azure Container Instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"data\": \"MNIST\",  \"method\" : \"keras\"}, \n",
    "                                               description='Predict MNIST with keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing the Deploy\n",
    "\n",
    "Now that we have configured the deploy, we can now execute the deploy.  This will take some time to complete (around 10 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(runtime= \"python\", \n",
    "                                   entry_script=\"score.py\",\n",
    "                                   conda_file=\"myenv.yml\")\n",
    "\n",
    "service = Model.deploy(workspace=ws, \n",
    "                       name='keras-mnist-svc', \n",
    "                       models=[amlModel], \n",
    "                       inference_config=inference_config, \n",
    "                       deployment_config=aciconfig)\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizing the Deployed Service\n",
    "\n",
    "Now that the service has been deployed, we can validate it using two different approaches:\n",
    "\n",
    "* Using the Azure ML Python SDK\n",
    "* Using it as a REST endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with the SDK\n",
    "\n",
    "If we have a reference to the `service` object, we can simply call `service.run` and pass in the data we want to test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Get a sample index\n",
    "sample_indices = np.random.permutation(test_images.shape[0])[0:1]\n",
    "\n",
    "# Structure input data\n",
    "test_samples = json.dumps({\"data\": test_images[sample_indices].tolist()})\n",
    "print(\"JSON Input: \" + test_samples)\n",
    "test_samples = bytes(test_samples, encoding='utf8')\n",
    "\n",
    "# Execute the predictions\n",
    "results = service.run(input_data=test_samples)\n",
    "\n",
    "# Utility function to display the result\n",
    "def display_result(image, result):\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    grid = plt.GridSpec(1, 3, wspace=0.4, hspace=0.3)\n",
    "    plt.subplot(grid[0, 0])\n",
    "    plt.imshow(image, cmap='gray_r')\n",
    "    print(\"\\n\\n\")\n",
    "    print(f'Predicted Value: {result[\"value\"]}')\n",
    "    print(f'Certainty: {str(result[\"certainty\"])}')\n",
    "    print(f'Raw Result: {str(result)}')\n",
    " \n",
    "# Show the results\n",
    "for i, val in enumerate(results):\n",
    "    display_result(test_images[sample_indices[i]], val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing via HTTP\n",
    "\n",
    "Now that we have tested via the SDK, we can now validate this using any tool that can call the REST endpoint.  We will leverage the `requests` module in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Get a sample index\n",
    "sample_indices = np.random.permutation(test_images.shape[0])[0:1]\n",
    "    \n",
    "# Structure input data\n",
    "test_samples = json.dumps({\"data\": test_images[sample_indices].tolist()})\n",
    "print(\"JSON Input: \" + test_samples)\n",
    "test_samples = bytes(test_samples, encoding='utf8')\n",
    "\n",
    "print(\"INPUT DATA: \", test_samples)\n",
    "\n",
    "# Set the header and perform a POST request\n",
    "headers = {'Content-Type':'application/json'}\n",
    "resp = requests.post(service.scoring_uri, test_samples, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "\n",
    "# Read the returned JSON data\n",
    "results = json.loads(resp.text)\n",
    "\n",
    "# Show the results\n",
    "for i, val in enumerate(results):\n",
    "    display_result(test_images[sample_indices[i]], val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
